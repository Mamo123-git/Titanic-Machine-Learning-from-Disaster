{"cells":[{"metadata":{},"cell_type":"markdown","source":"# The Challenge"},{"metadata":{},"cell_type":"markdown","source":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (ie name, age, gender, socio-economic class, etc)."},{"metadata":{},"cell_type":"markdown","source":"# Approach"},{"metadata":{},"cell_type":"markdown","source":"I have divided the entire step to analyze the data into three steps\n    \n    Exploratory Data Analysis\n    Feature Scaling\n    Applying Classification Model"},{"metadata":{},"cell_type":"markdown","source":"Importing all the necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To extract data from the csv file"},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/titanic/test.csv\")\ntrain = pd.read_csv(\"../input/titanic/train.csv\")\n\ny=train.Survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features = train.drop(['Survived'], axis=1)\ntest_features = test\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax= sns.countplot(train['Survived'])\nax.set_title('No of Passengers survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of survivals basis Gender"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x='Sex',hue ='Survived',data= train)\nax.set_title('Sex vs Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of survivals basis Pclass types"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x='Pclass',hue ='Survived',data= train)\nax.set_title('Pclass vs Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of survivals basis Embarked types"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x='Embarked',hue ='Survived',data= train)\nax.set_title('Embarked vs Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of survivals basis Sex, Age types"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train,split=True)\nax.set_title('Sex and Age vs Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of survivals basis Pclass, Sex types"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.violinplot(\"Pclass\",\"Sex\", hue=\"Survived\", data=train, split=True)\nax.set_title('Pclass and Sex vs Survived')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Counting the number of survivals basis SibSp types"},{"metadata":{"trusted":true},"cell_type":"code","source":"ax=sns.countplot(x='SibSp',hue ='Survived',data= train)\nax.set_title('SibSp vs Survived')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train[train['Pclass']==1].Fare)\n#ax[0].set_title('Fares in Pclass 1')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"g = sns.heatmap(train[[\"Age\",\"Fare\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\",\"Survived\"]].corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Scaling"},{"metadata":{},"cell_type":"markdown","source":"Dropping the feature Name,ticket,Cabin"},{"metadata":{"trusted":true},"cell_type":"code","source":"features= features.drop(['Name'],axis=1)\nfeatures= features.drop(['Ticket'],axis=1)\nfeatures= features.drop(['Cabin'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Finding the number of columns with NA"},{"metadata":{"trusted":true},"cell_type":"code","source":"features.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Replacing the NA values with mean values"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer()\nfeatures.iloc[:, [3,6]] = imp.fit_transform(features.iloc[:,[3,6]].values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features['Embarked'] = features['Embarked'].fillna('S')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfeatures['Sex']=le.fit_transform(features['Sex'])\nfeatures['Embarked']=le.fit_transform(features['Embarked'])\n\nfrom sklearn.preprocessing import OneHotEncoder\none = OneHotEncoder()\nfeatures = one.fit_transform(features).toarray()\nfeatures = pd.DataFrame(list(features))\n\nfrom sklearn.preprocessing import StandardScaler\nsd =StandardScaler()\nfeatures = sd.fit_transform(features)\nfeatures= pd.DataFrame(list(features))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"dividing the training and test sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train = features.iloc[:len(y),:].values\ntest = features.iloc[len(y):,:].values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Selecting the best classification model:"},{"metadata":{},"cell_type":"markdown","source":"Importing all the necessary Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Logistic Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"lin_reg = LogisticRegression()\nlin_reg.fit(train,y)\nlin_reg.score(train,y)\n\ny_pred = lin_reg.predict(train)\ntest_y = lin_reg.predict(test)\nlin_reg.score(test,test_y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Confusion matrix to check the scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, y_pred)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nprecision_score(y, y_pred)\nrecall_score(y, y_pred)\nf1_score(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### KNN ( K- Nearest Neighbour)"},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(train, y)\ny_pred = knn.predict(train)\n\nknn.score(train,y)\n\ny_test = knn.predict(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Confusion matrix to check the scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, y_pred)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nprecision_score(y, y_pred)\nrecall_score(y, y_pred)\nf1_score(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Gaussian Naive"},{"metadata":{"trusted":true},"cell_type":"code","source":"nb = GaussianNB()\nnb.fit(train, y)\n\ny_pred = nb.predict(train)\n\nnb.score(train,y)\n\ny_test = nb.predict(test)\n\nnb.score(test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using Confusion matrix to check the scores"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, y_pred)\n\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nprecision_score(y, y_pred)\nrecall_score(y, y_pred)\nf1_score(y, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Support Vector Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"svm = SVC()\nsvm.fit(train,y)\nsvm.score(train,y)\ny_test = svm.predict(test)\n\nsvm.score(test,y_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"trusted":true},"cell_type":"code","source":"dtf = DecisionTreeClassifier(criterion = 'entropy', max_depth = 11)\ndtf.fit(train,y)\ndtf.score(train,y)\n\ny_pred = dtf.predict(test)\ndtf.score(test,y_pred)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":1}